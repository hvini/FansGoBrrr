# ğŸ§  ANN from Scratch in C (CPU â†’ GPU)

## ğŸš§ Phase 1: CPU Implementation

- [ ] ğŸ”§ **Build System** â€“ Set up Makefile or CMake
- [ ] ğŸ§± **Data Structures** â€“ Define `Matrix`, `Layer`, `Network`
- [ ] â— **Matrix Operations** â€“ Add, multiply, transpose
- [ ] ğŸ” **Activation Functions** â€“ Sigmoid, ReLU, Softmax
- [ ] ğŸ”„ **Forward Propagation** â€“ Weighted sums + activations
- [ ] ğŸ“‰ **Loss Function** â€“ MSE or Cross-Entropy
- [ ] ğŸ§® **Backpropagation** â€“ Calculate gradients
- [ ] ğŸ‹ï¸ **Weight Updates** â€“ Apply SGD
- [ ] ğŸŒ€ **Training Loop** â€“ Run for N epochs
- [ ] âœ… **Test Cases (CPU)**
  - Matrix operations match expected results
  - Activation outputs correct for sample inputs
  - Forward propagation output sanity check
  - Backpropagation gradients validated numerically
  - Training on XOR dataset: loss decreases, correct outputs

---

## ğŸš€ Phase 2: GPU Acceleration (CUDA)

- [ ] âš™ï¸ **CUDA Setup** â€“ Configure build and test kernel
- [ ] ğŸ§Š **Matrix Ops (GPU)** â€“ Port add, multiply, etc.
- [ ] ğŸŒ **Activations (GPU)** â€“ Parallel element-wise ops
- [ ] ğŸ§¬ **Forward Prop (GPU)** â€“ Matrix ops + activations
- [ ] ğŸ”™ **Backprop (GPU)** â€“ Gradient calculation on GPU
- [ ] ğŸ **Training Loop (GPU)** â€“ Fully GPU-accelerated
- [ ] ğŸ” **Test Cases (GPU)**
  - GPU matrix ops produce identical results to CPU
  - Activation functions match CPU outputs
  - Forward propagation matches CPU outputs
  - Backpropagation gradients match CPU calculations
  - Training on XOR dataset converges with GPU
     
---
